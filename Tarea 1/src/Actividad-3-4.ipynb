{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oscar Esaú Peralta Rosales\n",
    "## Tarea 1: Fundamentos de Minería de Texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "from nltk.tokenize import WordPunctTokenizer \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 3: Detección de Agresividad con Análisis de Sentimiento Básico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos Parte 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carga de los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mex_corpus = CategorizedPlaintextCorpusReader('./data/corpus/', r'.*\\.txt', cat_pattern=r'(\\w+)/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer() \n",
    "stopw = stopwords.words('spanish') + stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [ \n",
    "            [token for token in tk.tokenize(tweet) if token not in stopw and len(token) > 2]\n",
    "            for tweet in mex_corpus.raw('mex_train.txt').split('\\n') if tweet\n",
    "          ]\n",
    "y_train = [int(label) for label in mex_corpus.raw('mex_train_labels.txt').split('\\n') if label ]\n",
    "x_val = [ \n",
    "            [token for token in tk.tokenize(tweet) if token not in stopw and len(token) > 2]\n",
    "            for tweet in mex_corpus.raw('mex_val.txt').split('\\n') if tweet\n",
    "        ]\n",
    "y_val = [int(label) for label in mex_corpus.raw('mex_val_labels.txt').split('\\n') if label ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Combine todo lo anterior en experimentos con una Bolsa de Palabras Tradicional con diferente pesado y observé si la clasificación mejora cuando se incorpora algo de lo anterior. Pruebe al menos tres pesados: binario, frecuencia normalizada y tfidf. Para construir la representación final del documento utilice la concatenación de todas representaciones anteriores (Bolsa de Palabras Normal + Bolsa de Sentimientos de Canada + Bolsa de Sentimientos de Grigori + Bolsa de PalabrasFoneticas), y aliméntelas a un SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading phonemes dict and build vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51637it [00:00, 129344.11it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_name = './data/phonemes_dict/es.csv'\n",
    "\n",
    "phonemes_map = {}\n",
    "phonemes_vocab = {}\n",
    "count = 0\n",
    "\n",
    "with open(file_name, newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    start = False\n",
    "    for index, row in enumerate(tqdm(spamreader)):\n",
    "        if index == 0:\n",
    "            continue\n",
    "        word = unidecode.unidecode(row[0]).lower()\n",
    "        phonemes_map[word] = [''.join(fn.split(' ')) for fn in row[1].split('ˈ') if fn]\n",
    "        # Add phoneme to the vocab\n",
    "        for pf in phonemes_map[word]:\n",
    "            if not pf in phonemes_vocab:\n",
    "                phonemes_vocab[pf] = count\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carga de diccionario de emociones y vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './data/NRC-Emotion-Lexicon-v0.92-In105Languages-Nov2017Translations.xlsx'\n",
    "df = pd.read_excel(file_name, usecols='CI,DB:DK')\n",
    "\n",
    "spzip = zip(np.array([x.lower() for x in np.array(df['Spanish (es)'])]), \n",
    "            np.array(df['Positive']),\n",
    "            np.array(df['Negative']),\n",
    "            np.array(df['Anger']),\n",
    "            np.array(df['Anticipation']),\n",
    "            np.array(df['Disgust']),\n",
    "            np.array(df['Fear']),\n",
    "            np.array(df['Joy']),\n",
    "            np.array(df['Sadness']),\n",
    "            np.array(df['Surprise']),\n",
    "            np.array(df['Trust']))\n",
    "\n",
    "spanish_map = sorted(spzip, key=lambda item:item[0])\n",
    "\n",
    "def spanish_map_search(spanish_map, word):\n",
    "    \"\"\"Returns a array with the emotions for any word\"\"\"\n",
    "    word = word.lower()\n",
    "    i = 0\n",
    "    j = len(spanish_map) - 1\n",
    "    \n",
    "    while i < j:\n",
    "        m = int((i+j)/2)\n",
    "        match = spanish_map[m][0].lower()\n",
    "        if match == word:\n",
    "            return np.array(spanish_map[m][1:])\n",
    "        if word > match:\n",
    "            i = m + 1\n",
    "        else:\n",
    "            j = m - 1\n",
    "    \n",
    "    return np.zeros(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Carga de Emociones SEL y vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './data/SEL/SEL.csv'\n",
    "\n",
    "with open(file_name) as fs:\n",
    "    sel = [line.split(',') for line in fs if line]\n",
    "\n",
    "sel_map = { unidecode.unidecode(item[1]).lower(): (item[7], float(item[6])) for item in sel[1:]}\n",
    "sel_vocab = dict(zip(set([item[1][0] for item in sel_map.items()]), range(6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vocabulario normal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(docs):\n",
    "    index = 0\n",
    "    vocabulary = {}\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = index\n",
    "                index += 1\n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = build_vocabulary(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construcción de la bolsa de emociones y fonemas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tradictional_bow(docs, _, vocab):\n",
    "    \"\"\" Build a emotions bag \"\"\"\n",
    "    bow = np.zeros((len(docs), len(vocab)), dtype=float)\n",
    "    \n",
    "    for index, doc in enumerate(tqdm(docs)):\n",
    "        for word in doc:\n",
    "            if not word in vocab:\n",
    "                continue\n",
    "            bow[index][vocab[word]] += 1\n",
    "    return bow\n",
    "\n",
    "\n",
    "def build_emotions_bow(docs, spanish_map, _, emotions=10):\n",
    "    \"\"\" Build a emotions bag \"\"\"\n",
    "    bow = np.zeros((len(docs), emotions), dtype=float)\n",
    "    \n",
    "    for index, doc in enumerate(tqdm(docs)):\n",
    "        for word in doc:\n",
    "            w_emotions = spanish_map_search(spanish_map, word)\n",
    "            bow[index] += w_emotions\n",
    "        \n",
    "    return bow\n",
    "\n",
    "\n",
    "def build_emotions_sel_bow(docs, sel_map, sel_vocab):\n",
    "    \"\"\" Build a emotions bag \"\"\"\n",
    "    bows = np.zeros((len(docs), len(sel_vocab)), dtype=float)\n",
    "    \n",
    "    for index, doc in enumerate(tqdm(docs)):\n",
    "        for _word in doc:\n",
    "            word = unidecode.unidecode(_word)\n",
    "            if not word in sel_map:\n",
    "                continue\n",
    "            # Increase by pfa\n",
    "            bows[index][sel_vocab[sel_map[word][0]]] += sel_map[word][1]\n",
    "            \n",
    "    return bows\n",
    "\n",
    "\n",
    "def build_phonemes_bow(docs, phonemes_map, phonemes_vocab):\n",
    "    \"\"\" Build a phonemes bag \"\"\"\n",
    "    bows = np.zeros((len(docs), len(phonemes_vocab)), dtype=float)\n",
    "    \n",
    "    for index, doc in enumerate(tqdm(docs)):\n",
    "        for _word in doc:\n",
    "            word = unidecode.unidecode(_word)\n",
    "            if not word in phonemes_map:\n",
    "                continue\n",
    "                \n",
    "            for phome in phonemes_map[word]:\n",
    "                w_index = phonemes_vocab[phome]\n",
    "                bows[index][w_index] += 1\n",
    "    return bows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construcción de bolsa {binaria, frecuencias, tfidf} de x tipo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_binary_bow(x, bow_builder, token_map, vocab_map):\n",
    "    \"\"\" Build a binary bow \"\"\"\n",
    "    bow = bow_builder(x, token_map, vocab_map)\n",
    "    bow[bow > 0] = 1\n",
    "    return bow\n",
    "\n",
    "\n",
    "def build_frecs_bow(x, bow_builder, token_map, vocab_map, normalize=False):\n",
    "    \"\"\" Build a frequencies bow \"\"\"\n",
    "    # The bow already has the frequencies\n",
    "    bow = bow_builder(x, token_map, vocab_map)\n",
    "    if normalize:\n",
    "        for row in bow:\n",
    "            row /= np.linalg.norm(row) or 1.0\n",
    "    return bow\n",
    "\n",
    "\n",
    "def build_tfidf_bow(x, bow_builder, token_map, vocab_map, normalize=False):\n",
    "    \"\"\" Build a tfidf bow \"\"\"\n",
    "    bows = bow_builder(x, token_map, vocab_map)\n",
    "    \n",
    "    # Compute count of terms aparitions on documents\n",
    "    ndocs_terms = np.sum(bows > 0, axis=0)\n",
    "    zeros = np.where(ndocs_terms == 0)[0]\n",
    "    ndocs_terms[zeros] = 1\n",
    "\n",
    "    for index, bow in enumerate(bows):\n",
    "        # compute tf\n",
    "        bow /= np.sum(bow > 0) or 1\n",
    "        # compute tf*idf\n",
    "        bow *= np.log(bows.shape[0] / ndocs_terms)\n",
    "        bow[zeros] = 0.0\n",
    "        if normalize:\n",
    "            bow /= np.linalg.norm(bow) or 1.0\n",
    "    return bows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clasificación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(x_train, y_train, x_val, y_val, kbest=None):\n",
    "    \"\"\" Clasificación con SVM, feature selection with chi2 \"\"\"\n",
    "    parameters = {'C': [.05, .12, .25, .5, 1, 2, 4]}\n",
    "    \n",
    "    if kbest:\n",
    "        selectk = SelectKBest(chi2, k=kbest)\n",
    "        selectk.fit(x_train, y_train)\n",
    "        x_train = selectk.transform(x_train)\n",
    "        x_val = selectk.transform(x_val)\n",
    "    \n",
    "    svr = svm.LinearSVC(class_weight='balanced')\n",
    "    grid = GridSearchCV(estimator=svr, param_grid=parameters, n_jobs=8, scoring=\"f1_macro\", cv=5)\n",
    "    \n",
    "    grid.fit(x_train, y_train) \n",
    "\n",
    "    y_pred = grid.predict(x_val)\n",
    "\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_val, y_pred, average='macro', pos_label=None)\n",
    "    a = accuracy_score(y_val, y_pred)\n",
    "    print(confusion_matrix(y_val, y_pred) )\n",
    "    print(metrics.classification_report(y_val, y_pred))\n",
    "    return p, r , f, a\n",
    "\n",
    "metrics_hist = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construcción de diferenties tipos de bolsas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5544/5544 [00:00<00:00, 37144.06it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 54259.85it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 50991.45it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 55401.22it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 51769.43it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 16535.84it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 34884.47it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 12726.54it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 34101.18it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 15057.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Bolsas vocabulario tradicional\n",
    "x_train_trad_binary = build_binary_bow(x_train, build_tradictional_bow, _, vocabulary)\n",
    "x_val_trad_binary = build_binary_bow(x_val, build_tradictional_bow, _, vocabulary)\n",
    "\n",
    "x_train_trad_frec = build_frecs_bow(x_train, build_tradictional_bow, _, vocabulary, normalize=False)\n",
    "x_val_trad_frec = build_frecs_bow(x_val, build_tradictional_bow, _, vocabulary, normalize=False)\n",
    "\n",
    "x_train_trad_frec_norm = build_frecs_bow(x_train, build_tradictional_bow, _, vocabulary, normalize=True)\n",
    "x_val_trad_frec_norm = build_frecs_bow(x_val, build_tradictional_bow, _, vocabulary, normalize=True)\n",
    "\n",
    "x_train_trad_tfidf = build_tfidf_bow(x_train, build_tradictional_bow, _, vocabulary, normalize=True)\n",
    "x_val_trad_tfidf = build_tfidf_bow(x_val, build_tradictional_bow, _, vocabulary, normalize=True)\n",
    "\n",
    "x_train_trad_tfidf_norm = build_tfidf_bow(x_train, build_tradictional_bow, _, vocabulary, normalize=True)\n",
    "x_val_trad_tfidf_norm = build_tfidf_bow(x_val, build_tradictional_bow, _, vocabulary, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5544/5544 [00:00<00:00, 8175.01it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 8788.93it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 8723.82it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 8993.16it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 8463.04it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 8688.94it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 8647.30it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 8695.61it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 8725.29it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 8824.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Bolsas vocabulario emociones\n",
    "x_train_emot_binary = build_binary_bow(x_train, build_emotions_bow, spanish_map, _)\n",
    "x_val_emot_binary = build_binary_bow(x_val, build_emotions_bow, spanish_map, _)\n",
    "\n",
    "x_train_emot_frec = build_frecs_bow(x_train, build_emotions_bow, spanish_map, _, normalize=False)\n",
    "x_val_emot_frec = build_frecs_bow(x_val, build_emotions_bow, spanish_map, _, normalize=False)\n",
    "\n",
    "x_train_emot_frec_norm = build_frecs_bow(x_train, build_emotions_bow, spanish_map, _, normalize=True)\n",
    "x_val_emot_frec_norm = build_frecs_bow(x_val, build_emotions_bow, spanish_map, _, normalize=True)\n",
    "\n",
    "x_train_emot_tfidf = build_tfidf_bow(x_train, build_emotions_bow, spanish_map, _, normalize=True)\n",
    "x_val_emot_tfidf = build_tfidf_bow(x_val, build_emotions_bow, spanish_map, _, normalize=True)\n",
    "\n",
    "x_train_emot_tfidf_norm = build_tfidf_bow(x_train, build_emotions_bow, spanish_map, _, normalize=True)\n",
    "x_val_emot_tfidf_norm = build_tfidf_bow(x_val, build_emotions_bow, spanish_map, _, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5544/5544 [00:00<00:00, 53156.66it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 94007.10it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 91181.24it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 79179.04it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 91178.38it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 92915.14it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 93538.20it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 90764.11it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 95991.30it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 89593.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Bolsas vocabulario emociones SEL\n",
    "x_train_emot_sel_binary = build_binary_bow(x_train, build_emotions_sel_bow, sel_map, sel_vocab)\n",
    "x_val_emot_sel_binary = build_binary_bow(x_val, build_emotions_sel_bow, sel_map, sel_vocab)\n",
    "\n",
    "x_train_emot_sel_frec = build_frecs_bow(x_train, build_emotions_sel_bow, sel_map, sel_vocab, normalize=False)\n",
    "x_val_emot_sel_frec = build_frecs_bow(x_val, build_emotions_sel_bow, sel_map, sel_vocab, normalize=False)\n",
    "\n",
    "x_train_emot_sel_frec_norm = build_frecs_bow(x_train, build_emotions_sel_bow, sel_map, sel_vocab, normalize=True)\n",
    "x_val_emot_sel_frec_norm = build_frecs_bow(x_val, build_emotions_sel_bow, sel_map, sel_vocab, normalize=True)\n",
    "\n",
    "x_train_emot_sel_tfidf = build_tfidf_bow(x_train, build_emotions_sel_bow, sel_map, sel_vocab, normalize=True)\n",
    "x_val_emot_sel_tfidf = build_tfidf_bow(x_val, build_emotions_sel_bow, sel_map, sel_vocab, normalize=True)\n",
    "\n",
    "x_train_emot_sel_tfidf_norm = build_tfidf_bow(x_train, build_emotions_sel_bow, sel_map, sel_vocab, normalize=True)\n",
    "x_val_emot_sel_tfidf_norm = build_tfidf_bow(x_val, build_emotions_sel_bow, sel_map, sel_vocab, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5544/5544 [00:00<00:00, 29604.63it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 34423.97it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 34038.19it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 34614.91it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 33953.00it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 31362.33it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 26546.62it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 20655.32it/s]\n",
      "100%|██████████| 5544/5544 [00:00<00:00, 28118.78it/s]\n",
      "100%|██████████| 616/616 [00:00<00:00, 19979.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# Bolsas vocabulario emociones SEL\n",
    "x_train_phonemes_binary = build_binary_bow(x_train, build_phonemes_bow, phonemes_map, phonemes_vocab)\n",
    "x_val_phonemes_binary = build_binary_bow(x_val, build_phonemes_bow, phonemes_map, phonemes_vocab)\n",
    "\n",
    "x_train_phonemes_frec = build_frecs_bow(x_train, build_phonemes_bow, phonemes_map, phonemes_vocab, normalize=False)\n",
    "x_val_phonemes_frec = build_frecs_bow(x_val, build_phonemes_bow, phonemes_map, phonemes_vocab, normalize=False)\n",
    "\n",
    "x_train_phonemes_frec_norm = build_frecs_bow(x_train, build_phonemes_bow, phonemes_map, phonemes_vocab, normalize=True)\n",
    "x_val_phonemes_frec_norm = build_frecs_bow(x_val, build_phonemes_bow, phonemes_map, phonemes_vocab, normalize=True)\n",
    "\n",
    "x_train_phonemes_tfidf = build_tfidf_bow(x_train, build_phonemes_bow, phonemes_map, phonemes_vocab, normalize=True)\n",
    "x_val_phonemes_tfidf = build_tfidf_bow(x_val, build_phonemes_bow, phonemes_map, phonemes_vocab, normalize=True)\n",
    "\n",
    "x_train_phonemes_tfidf_norm = build_tfidf_bow(x_train, build_phonemes_bow, phonemes_map, phonemes_vocab, normalize=True)\n",
    "x_val_phonemes_tfidf_norm = build_tfidf_bow(x_val, build_phonemes_bow, phonemes_map, phonemes_vocab, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bolsa tradicional + emociones, binaria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[341  56]\n",
      " [ 68 151]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.85       397\n",
      "           1       0.73      0.69      0.71       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.78      0.77      0.78       616\n",
      "weighted avg       0.80      0.80      0.80       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nx_train = np.concatenate((x_train_trad_binary, x_train_emot_binary, x_train_emot_sel_binary), axis=1)\n",
    "nx_val = np.concatenate((x_val_trad_binary, x_val_emot_binary, x_val_emot_sel_binary), axis=1)\n",
    "metrics_hist.append((\"Bolsa tradicional + emociones, binaria\", \n",
    "                     *classify(nx_train, y_train, nx_val, y_val, kbest=1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bolsa tradiciones + emociones, frecuencias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[339  58]\n",
      " [ 65 154]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85       397\n",
      "           1       0.73      0.70      0.71       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.78      0.78      0.78       616\n",
      "weighted avg       0.80      0.80      0.80       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nx_train = np.concatenate((x_train_trad_frec, x_train_emot_frec, x_train_emot_sel_frec), axis=1)\n",
    "nx_val = np.concatenate((x_val_trad_frec, x_val_emot_frec, x_val_emot_sel_frec), axis=1)\n",
    "metrics_hist.append((\"Bolsa tradiciones + emociones, frecuencias\", \n",
    "                     *classify(nx_train, y_train, nx_val, y_val, kbest=1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bolsa tradiciones + emociones, frecuencias normalizadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[335  62]\n",
      " [ 63 156]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84       397\n",
      "           1       0.72      0.71      0.71       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.78      0.78      0.78       616\n",
      "weighted avg       0.80      0.80      0.80       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nx_train = np.concatenate((x_train_trad_frec_norm, x_train_emot_frec_norm, x_train_emot_sel_frec_norm), axis=1)\n",
    "nx_val = np.concatenate((x_val_trad_frec_norm, x_val_emot_frec_norm, x_val_emot_sel_frec_norm), axis=1)\n",
    "metrics_hist.append((\"Bolsa tradiciones + emociones, frecuencias norma\", \n",
    "                     *classify(nx_train, y_train, nx_val, y_val, kbest=1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bolsa tradiciones + emociones, tfidf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[330  67]\n",
      " [ 63 156]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.84       397\n",
      "           1       0.70      0.71      0.71       219\n",
      "\n",
      "    accuracy                           0.79       616\n",
      "   macro avg       0.77      0.77      0.77       616\n",
      "weighted avg       0.79      0.79      0.79       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nx_train = np.concatenate((x_train_trad_tfidf, x_train_emot_tfidf, x_train_emot_sel_tfidf), axis=1)\n",
    "nx_val = np.concatenate((x_val_trad_tfidf, x_val_emot_tfidf, x_val_emot_sel_tfidf), axis=1)\n",
    "metrics_hist.append((\"Bolsa tradiciones + emociones, tfidf\", \n",
    "                     *classify(nx_train, y_train, nx_val, y_val, kbest=1500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bolsa tradiciones + emociones, tfidf norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[330  67]\n",
      " [ 63 156]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.84       397\n",
      "           1       0.70      0.71      0.71       219\n",
      "\n",
      "    accuracy                           0.79       616\n",
      "   macro avg       0.77      0.77      0.77       616\n",
      "weighted avg       0.79      0.79      0.79       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nx_train = np.concatenate((x_train_trad_tfidf_norm, x_train_emot_tfidf_norm, x_train_emot_sel_tfidf_norm), axis=1)\n",
    "nx_val = np.concatenate((x_val_trad_tfidf_norm, x_val_emot_tfidf_norm, x_val_emot_sel_tfidf_norm), axis=1)\n",
    "metrics_hist.append((\"Bolsa tradiciones + emociones, tfidf norm\", \n",
    "                     *classify(nx_train, y_train, nx_val, y_val, kbest=1500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bolsa phonemes + emociones, binaria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[313  84]\n",
      " [105 114]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.79      0.77       397\n",
      "           1       0.58      0.52      0.55       219\n",
      "\n",
      "    accuracy                           0.69       616\n",
      "   macro avg       0.66      0.65      0.66       616\n",
      "weighted avg       0.69      0.69      0.69       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nx_train = np.concatenate((x_train_phonemes_binary, x_train_emot_binary, x_train_emot_sel_binary), axis=1)\n",
    "nx_val = np.concatenate((x_val_phonemes_binary, x_val_emot_binary, x_val_emot_sel_binary), axis=1)\n",
    "metrics_hist.append((\"Bolsa phonemes + emociones, binaria\", \n",
    "                     *classify(nx_train, y_train, nx_val, y_val, kbest=2000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bolsa phonemes + emociones, frecuencias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[308  89]\n",
      " [ 99 120]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77       397\n",
      "           1       0.57      0.55      0.56       219\n",
      "\n",
      "    accuracy                           0.69       616\n",
      "   macro avg       0.67      0.66      0.66       616\n",
      "weighted avg       0.69      0.69      0.69       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nx_train = np.concatenate((x_train_phonemes_frec, x_train_emot_frec, x_train_emot_sel_frec), axis=1)\n",
    "nx_val = np.concatenate((x_val_phonemes_frec, x_val_emot_frec, x_val_emot_sel_frec), axis=1)\n",
    "metrics_hist.append((\"Bolsa phonemes + emociones, frecuencias\", \n",
    "                     *classify(nx_train, y_train, nx_val, y_val, kbest=5000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bolsa phonemes + emociones, frecuencias normalizadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[308  89]\n",
      " [107 112]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.78      0.76       397\n",
      "           1       0.56      0.51      0.53       219\n",
      "\n",
      "    accuracy                           0.68       616\n",
      "   macro avg       0.65      0.64      0.65       616\n",
      "weighted avg       0.68      0.68      0.68       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nx_train = np.concatenate((x_train_phonemes_frec_norm, x_train_emot_frec_norm, x_train_emot_sel_frec_norm), axis=1)\n",
    "nx_val = np.concatenate((x_val_phonemes_frec_norm, x_val_emot_frec_norm, x_val_emot_sel_frec_norm), axis=1)\n",
    "metrics_hist.append((\"Bolsa phonemes + emociones, frecuencias norma\", \n",
    "                     *classify(nx_train, y_train, nx_val, y_val, kbest=1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bolsa phonemes + emociones, tfidf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[308  89]\n",
      " [ 96 123]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77       397\n",
      "           1       0.58      0.56      0.57       219\n",
      "\n",
      "    accuracy                           0.70       616\n",
      "   macro avg       0.67      0.67      0.67       616\n",
      "weighted avg       0.70      0.70      0.70       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nx_train = np.concatenate((x_train_phonemes_tfidf, x_train_emot_tfidf, x_train_emot_sel_tfidf), axis=1)\n",
    "nx_val = np.concatenate((x_val_phonemes_tfidf, x_val_emot_tfidf, x_val_emot_sel_tfidf), axis=1)\n",
    "metrics_hist.append((\"Bolsa phonemes + emociones, tfidf\", \n",
    "                     *classify(nx_train, y_train, nx_val, y_val, kbest=1500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bolsa phonemes + emociones, tfidf norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[308  89]\n",
      " [ 96 123]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77       397\n",
      "           1       0.58      0.56      0.57       219\n",
      "\n",
      "    accuracy                           0.70       616\n",
      "   macro avg       0.67      0.67      0.67       616\n",
      "weighted avg       0.70      0.70      0.70       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nx_train = np.concatenate((x_train_phonemes_tfidf_norm, x_train_emot_tfidf_norm, x_train_emot_sel_tfidf_norm), axis=1)\n",
    "nx_val = np.concatenate((x_val_phonemes_tfidf_norm, x_val_emot_tfidf_norm, x_val_emot_sel_tfidf_norm), axis=1)\n",
    "metrics_hist.append((\"Bolsa phonemes + emociones, tfidf norm\", \n",
    "                     *classify(nx_train, y_train, nx_val, y_val, kbest=1500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tabla comparativa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Fscore</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bolsa tradicional + emociones, binaria</td>\n",
       "      <td>0.781605</td>\n",
       "      <td>0.774220</td>\n",
       "      <td>0.777537</td>\n",
       "      <td>0.798701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bolsa tradiciones + emociones, frecuencias</td>\n",
       "      <td>0.782762</td>\n",
       "      <td>0.778550</td>\n",
       "      <td>0.780530</td>\n",
       "      <td>0.800325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bolsa tradiciones + emociones, frecuencias norma</td>\n",
       "      <td>0.778652</td>\n",
       "      <td>0.778079</td>\n",
       "      <td>0.778363</td>\n",
       "      <td>0.797078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bolsa tradiciones + emociones, tfidf</td>\n",
       "      <td>0.769623</td>\n",
       "      <td>0.771782</td>\n",
       "      <td>0.770663</td>\n",
       "      <td>0.788961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bolsa tradiciones + emociones, tfidf norm</td>\n",
       "      <td>0.769623</td>\n",
       "      <td>0.771782</td>\n",
       "      <td>0.770663</td>\n",
       "      <td>0.788961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bolsa phonemes + emociones, binaria</td>\n",
       "      <td>0.662281</td>\n",
       "      <td>0.654481</td>\n",
       "      <td>0.657430</td>\n",
       "      <td>0.693182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bolsa phonemes + emociones, frecuencias</td>\n",
       "      <td>0.665460</td>\n",
       "      <td>0.661882</td>\n",
       "      <td>0.663458</td>\n",
       "      <td>0.694805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bolsa phonemes + emociones, frecuencias norma</td>\n",
       "      <td>0.649691</td>\n",
       "      <td>0.643617</td>\n",
       "      <td>0.645977</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bolsa phonemes + emociones, tfidf</td>\n",
       "      <td>0.671282</td>\n",
       "      <td>0.668731</td>\n",
       "      <td>0.669902</td>\n",
       "      <td>0.699675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bolsa phonemes + emociones, tfidf norm</td>\n",
       "      <td>0.671282</td>\n",
       "      <td>0.668731</td>\n",
       "      <td>0.669902</td>\n",
       "      <td>0.699675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Embedding  Precision    Recall  \\\n",
       "0            Bolsa tradicional + emociones, binaria   0.781605  0.774220   \n",
       "1        Bolsa tradiciones + emociones, frecuencias   0.782762  0.778550   \n",
       "2  Bolsa tradiciones + emociones, frecuencias norma   0.778652  0.778079   \n",
       "3              Bolsa tradiciones + emociones, tfidf   0.769623  0.771782   \n",
       "4         Bolsa tradiciones + emociones, tfidf norm   0.769623  0.771782   \n",
       "5               Bolsa phonemes + emociones, binaria   0.662281  0.654481   \n",
       "6           Bolsa phonemes + emociones, frecuencias   0.665460  0.661882   \n",
       "7     Bolsa phonemes + emociones, frecuencias norma   0.649691  0.643617   \n",
       "8                 Bolsa phonemes + emociones, tfidf   0.671282  0.668731   \n",
       "9            Bolsa phonemes + emociones, tfidf norm   0.671282  0.668731   \n",
       "\n",
       "     Fscore  Accuracy  \n",
       "0  0.777537  0.798701  \n",
       "1  0.780530  0.800325  \n",
       "2  0.778363  0.797078  \n",
       "3  0.770663  0.788961  \n",
       "4  0.770663  0.788961  \n",
       "5  0.657430  0.693182  \n",
       "6  0.663458  0.694805  \n",
       "7  0.645977  0.681818  \n",
       "8  0.669902  0.699675  \n",
       "9  0.669902  0.699675  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame(data=metrics_hist, columns = ['Embedding', 'Precision', 'Recall', 'Fscore', 'Accuracy'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante el uso de enmascaramiento con los recursos léxicos de emociones se observaron métricas más bajas que el solo usar la bolsa de palabras tradicional.\n",
    "Se intuye que es debido a la amplitud del vocabulario usado de estos recursos, puesto que hay tweets cuyas palabras que no tienen ningun match con una emocíón y por tanto su bow contenía ceros, reduciendo la información bastante la información capturada.\n",
    "\n",
    "Se implementaron algunas mejoras para mejorar las representaciones en bolsas de palabras como\n",
    "\n",
    "1. Uso de stop words\n",
    "2. Normalizar palabras a minúsculas\n",
    "3. Remover tildes\n",
    "\n",
    "Unas posibles mejoras a implementarse sería usar lemmatization o stemming y así poder reducir los ceros de los match con las emociones debido a alguna otra conjugación o una palabra y al mejor uso de la raices de estas.\n",
    "\n",
    "También se implementó la selección de mejores caracteristicas usando CHI2 mediante la biblioteca de sklearn.\n",
    "\n",
    "No se observaron muchas mejoras significativas con respecto a los ejercicios realizados en la práctica 3, en general integrar la bolsa de emociones junto con la representación tradicional de bolsa de palabras o la de fonemas no mejoro mucho. Pero se observa un mejor comportamiento usando la bolsa de palabras tradicional vs la bolsa de palabras de fonemas. Por otro lado a pesar de su simplicidad la bolsa de palabras binarias sigue teniando muy buenos resultados.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
